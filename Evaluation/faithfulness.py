import logging
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict, Any, Optional

import pandas as pd
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.evaluation import context_relevancy
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.llms.google_genai import GoogleGenAI
from ragas import EvaluationDataset
from ragas.integrations.llama_index import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision

from env_config import settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class EvaluationConfig:
    model_api_key: str
    documents_path: Optional[str] = None
    output_csv_path: str = "rag_evaluation_results.csv"
    model_name: str = "gemini-2.5-flash"

    chunk_size: int = 1024
    chunk_overlap: int = 20

    semantic_breakpoint_percentile_threshold: int = 95
    semantic_buffer_size: int = 1
    # semantic_embedding_model: str = "text-embedding-ada-002"
    semantic_embedding_model: str = "gemini-embedding-004"

    sentence_chunk_size: int = 1024
    sentence_chunk_overlap: int = 200

    token_chunk_size: int = 512
    token_chunk_overlap: int = 50


@dataclass
class EvaluationData:
    """Dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ bao gá»“m cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i tham chiáº¿u"""

    questions: List[str]
    reference_answers: List[List[str]]

    def __post_init__(self):
        if len(self.questions) != len(self.reference_answers):
            raise ValueError(
                "Sá»‘ lÆ°á»£ng cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i tham chiáº¿u pháº£i báº±ng nhau"
            )


def get_all_metrics() -> List:
    """
    Láº¥y táº¥t cáº£ metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡

    Returns:
        List: Danh sÃ¡ch cÃ¡c metrics
    """
    return [
        faithfulness,
        answer_relevancy,
        context_relevancy,
        context_recall,
        context_precision,
    ]


class RAGEvaluator:
    """Class chÃ­nh Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ há»‡ thá»‘ng RAG"""

    def __init__(self, config: EvaluationConfig):
        self.config = config
        self.query_engine: Optional[BaseQueryEngine] = None
        self.embedding_model = GoogleGenAIEmbedding(
            api_key=settings.AI_API_KEY,
            model_name=settings.EMBEDDING_MODEL_NAME)
        self.llm  = GoogleGenAI(api_key=settings.AI_API_KEY)
        self.build_rag_system(config.documents_path)


    def _create_node_parser(self):
        """
        Táº¡o node parser dá»±a trÃªn cáº¥u hÃ¬nh chunking strategy

        Returns:
            Node parser phÃ¹ há»£p vá»›i strategy Ä‘Æ°á»£c chá»n
        """

        return SemanticSplitterNodeParser(
            embed_model=self.embedding_model
        )

    def build_rag_system(self, documents_path: str = None) -> Optional[BaseQueryEngine]:
        """
        XÃ¢y dá»±ng há»‡ thá»‘ng RAG tá»« thÆ° má»¥c tÃ i liá»‡u vá»›i semantic chunking

        Args:
            documents_path: ÄÆ°á»ng dáº«n tá»›i thÆ° má»¥c chá»©a tÃ i liá»‡u

        Returns:
            BaseQueryEngine: Engine Ä‘á»ƒ thá»±c hiá»‡n truy váº¥n
        """
        try:
            doc_path = documents_path or self.config.documents_path
            if not doc_path:
                raise ValueError("ChÆ°a cung cáº¥p Ä‘Æ°á»ng dáº«n tÃ i liá»‡u")

            logger.info(f"Äang Ä‘á»c tÃ i liá»‡u tá»«: {doc_path}")
            documents = SimpleDirectoryReader(input_files=[doc_path]).load_data()
            logger.info(f"ÄÃ£ Ä‘á»c {len(documents)} tÃ i liá»‡u")

            node_parser = self._create_node_parser()
            logger.info(f"Sá»­ dá»¥ng semantic chunking vá»›i mÃ´ hÃ¬nh: {self.config.semantic_embedding_model}")

            nodes = node_parser.build_semantic_nodes_from_documents(documents)
            logger.info(f"ÄÃ£ táº¡o {len(nodes)} chunks tá»« tÃ i liá»‡u")

            # Log thÃ´ng tin vá» kÃ­ch thÆ°á»›c chunks (Ä‘á»ƒ debug)
            if nodes:
                chunk_sizes = [len(node.text) for node in nodes[:5]]  # Láº¥y 5 chunks Ä‘áº§u
                logger.info(f"KÃ­ch thÆ°á»›c chunks máº«u: {chunk_sizes}")

            # Táº¡o VectorStoreIndex tá»« nodes
            index = VectorStoreIndex(nodes, embed_model=self.embedding_model)

            self.query_engine = index.as_query_engine(llm=self.llm)
            logger.info("ÄÃ£ xÃ¢y dá»±ng thÃ nh cÃ´ng há»‡ thá»‘ng RAG vá»›i semantic chunking")

            return self.query_engine

        except Exception as ex:
            logger.error(f"Lá»—i khi xÃ¢y dá»±ng há»‡ thá»‘ng RAG: {ex}")
            raise

    def create_sample_evaluation_data(self) -> EvaluationData:
        """
        Táº¡o dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ máº«u - cÃ³ thá»ƒ thay tháº¿ báº±ng dá»¯ liá»‡u thá»±c táº¿

        Returns:
            EvaluationData: Dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ máº«u
        """
        sample_questions = [
            "Báº¡n cÃ³ thá»ƒ cung cáº¥p mÃ´ táº£ ngáº¯n gá»n vá» mÃ´ hÃ¬nh TinyLlama khÃ´ng?",
            "TÃ´i muá»‘n biáº¿t vá» cÃ¡c tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™ mÃ  TinyLlama Ä‘Ã£ thá»±c hiá»‡n.",
            "Táº¡i sao TinyLlama sá»­ dá»¥ng Grouped-query Attention?",
            "MÃ´ hÃ¬nh TinyLlama cÃ³ pháº£i lÃ  mÃ£ nguá»“n má»Ÿ khÃ´ng?",
            "HÃ£y cho tÃ´i biáº¿t vá» táº­p dá»¯ liá»‡u starcoderdata",
        ]

        sample_answers = [
            [
                "TinyLlama lÃ  má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhá» gá»n 1.1B Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn khoáº£ng 1 nghÃ¬n tá»· token trong khoáº£ng 3 epoch. Dá»±a trÃªn kiáº¿n trÃºc vÃ  tokenizer cá»§a Llama 2, TinyLlama táº­n dá»¥ng cÃ¡c tiáº¿n bá»™ tá»« cá»™ng Ä‘á»“ng mÃ£ nguá»“n má»Ÿ nhÆ° FlashAttention, Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u quáº£ tÃ­nh toÃ¡n tá»‘t hÆ¡n."
            ],
            [
                "Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, codebase Ä‘Ã£ tÃ­ch há»£p FSDP Ä‘á»ƒ táº­n dá»¥ng hiá»‡u quáº£ thiáº¿t láº­p multi-GPU vÃ  multi-node. Má»™t cáº£i tiáº¿n quan trá»ng khÃ¡c lÃ  tÃ­ch há»£p Flash Attention, má»™t cÆ¡ cháº¿ attention Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a."
            ],
            [
                "Äá»ƒ giáº£m overhead bÄƒng thÃ´ng bá»™ nhá»› vÃ  tÄƒng tá»‘c Ä‘á»™ suy luáº­n, chÃºng tÃ´i sá»­ dá»¥ng grouped-query attention trong mÃ´ hÃ¬nh. ChÃºng tÃ´i cÃ³ 32 head cho query attention vÃ  sá»­ dá»¥ng 4 nhÃ³m key-value head."
            ],
            ["CÃ³, TinyLlama lÃ  mÃ£ nguá»“n má»Ÿ"],
            [
                "Táº­p dá»¯ liá»‡u nÃ y Ä‘Æ°á»£c thu tháº­p Ä‘á»ƒ huáº¥n luyá»‡n StarCoder, má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›p lá»›n mÃ£ nguá»“n má»Ÿ máº¡nh máº½. NÃ³ bao gá»“m khoáº£ng 250 tá»· token trÃªn 86 ngÃ´n ngá»¯ láº­p trÃ¬nh."
            ],
        ]

        return EvaluationData(
            questions=sample_questions, reference_answers=sample_answers
        )

    def evaluate_rag_system(
            self, evaluation_data: EvaluationData, metrics: Optional[List] = None
    ) -> pd.DataFrame:
        """
        ÄÃ¡nh giÃ¡ há»‡ thá»‘ng RAG vá»›i dá»¯ liá»‡u vÃ  metrics Ä‘Æ°á»£c cung cáº¥p

        Args:
            evaluation_data: Dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡
            metrics: Danh sÃ¡ch metrics (náº¿u None sáº½ sá»­ dá»¥ng táº¥t cáº£)

        Returns:
            pd.DataFrame: Káº¿t quáº£ Ä‘Ã¡nh giÃ¡
        """

        if not self.query_engine:
            raise ValueError(
                "ChÆ°a xÃ¢y dá»±ng há»‡ thá»‘ng RAG. Gá»i build_rag_system() trÆ°á»›c."
            )

        if metrics is None:
            metrics = get_all_metrics()

        logger.info(f"Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ vá»›i {len(evaluation_data.questions)} cÃ¢u há»i")
        logger.info(f"Sá»­ dá»¥ng {len(metrics)} metrics")

        try:


            dataset = EvaluationDataset(evaluation_data)




            # Thá»±c hiá»‡n Ä‘Ã¡nh giÃ¡
            result = evaluate(
                query_engine=self.query_engine,
                dataset=dataset,
                metrics=metrics,
            )

            # Chuyá»ƒn Ä‘á»•i káº¿t quáº£ thÃ nh DataFrame
            df_result = result.to_pandas()

            # ThÃªm timestamp
            df_result["evaluation_timestamp"] = datetime.now()

            logger.info("ÄÃ¡nh giÃ¡ hoÃ n thÃ nh thÃ nh cÃ´ng")
            return df_result

        except Exception as e:
            logger.error(f"Lá»—i trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡: {e}")
            raise

    def save_results(self, results: pd.DataFrame, output_path: Optional[str] = None):
        """
        LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ ra file CSV

        Args:
            results: DataFrame chá»©a káº¿t quáº£
            output_path: ÄÆ°á»ng dáº«n file output (náº¿u None sáº½ dÃ¹ng config)
        """
        output_file = output_path or self.config.output_csv_path

        try:
            results.to_csv(output_file, sep=",", index=False, encoding="utf-8")
            logger.info(f"ÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o: {output_file}")
        except Exception as e:
            logger.error(f"Lá»—i khi lÆ°u file: {e}")
            raise

    def generate_evaluation_report(self, results: pd.DataFrame) -> Dict[str, Any]:
        """
        Táº¡o bÃ¡o cÃ¡o tÃ³m táº¯t káº¿t quáº£ Ä‘Ã¡nh giÃ¡

        Args:
            results: DataFrame chá»©a káº¿t quáº£ Ä‘Ã¡nh giÃ¡

        Returns:
            Dict: BÃ¡o cÃ¡o tÃ³m táº¯t
        """
        numeric_columns = results.select_dtypes(include=["float64", "int64"]).columns

        report = {
            "overview": {
                "total_questions": len(results),
                "evaluation_date": datetime.now().isoformat(),
                "metrics_evaluated": list(numeric_columns),
            },
            "summary_statistics": {
                col: {
                    "mean": float(results[col].mean()),
                    "std": float(results[col].std()),
                    "min": float(results[col].min()),
                    "max": float(results[col].max()),
                    "median": float(results[col].median()),
                }
                for col in numeric_columns
            },
            "performance_insights": self._analyze_performance(
                results, list(numeric_columns)
            ),
        }

        return report

    def _analyze_performance(
            self, results: pd.DataFrame, metric_columns: List[str]
    ) -> Dict[str, str]:
        """
        PhÃ¢n tÃ­ch hiá»‡u suáº¥t dá»±a trÃªn káº¿t quáº£

        Args:
            results: DataFrame chá»©a káº¿t quáº£
            metric_columns: Danh sÃ¡ch tÃªn cá»™t metrics

        Returns:
            Dict: PhÃ¢n tÃ­ch hiá»‡u suáº¥t
        """
        insights = {}

        for col in metric_columns:
            mean_score = results[col].mean()
            if mean_score >= 0.8:
                insights[col] = "Excellent performance"
            elif mean_score >= 0.6:
                insights[col] = "Good performance"
            elif mean_score >= 0.4:
                insights[col] = "Average performance - needs improvement"
            else:
                insights[col] = "Poor performance - requires significant improvement"

        return insights


def run_sample_evaluation():
    """
    Cháº¡y Ä‘Ã¡nh giÃ¡ máº«u vá»›i dá»¯ liá»‡u demo

    ÄÃ¢y lÃ  hÃ m demo Ä‘á»ƒ kiá»ƒm tra hoáº¡t Ä‘á»™ng cá»§a framework
    """
    # Cáº¥u hÃ¬nh Ä‘Ã¡nh giÃ¡
    config = EvaluationConfig(
        model_api_key=settings.AI_API_KEY,
        documents_path="document.pdf",
        output_csv_path="sample_rag_evaluation.csv",
        semantic_embedding_model=settings.EMBEDDING_MODEL_NAME,
    )

    try:
        evaluator = RAGEvaluator(config)

        eval_data = evaluator.create_sample_evaluation_data()
        evaluation_result = evaluator.evaluate_rag_system(eval_data)
        evaluator.save_results(evaluation_result)

        # Hiá»ƒn thá»‹ cÃ¢u há»i máº«u
        print("\nğŸ“‹ CÃ¢u há»i máº«u:")
        for i, question in enumerate(eval_data.questions[:3], 1):
            print(f"   {i}. {question}")
        print("   ...")

    except Exception as e:
        print(f"âŒ Lá»—i: {e}")


if __name__ == "__main__":
    print("ğŸš€ RAG Evaluation Framework")
    print("=" * 50)
    run_sample_evaluation()
