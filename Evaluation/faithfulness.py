import logging
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict, Any, Optional

import pandas as pd
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.google_genai import GoogleGenAI
from ragas import EvaluationDataset
from ragas.integrations.llama_index import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision

from env_config import settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class EvaluationConfig:
    model_api_key: str
    documents_path: Optional[str] = None
    output_csv_path: str = "rag_evaluation_results.csv"
    model_name: str = "gemini-2.5-flash"

    chunk_size: int = 1024
    chunk_overlap: int = 20

    semantic_breakpoint_percentile_threshold: int = 95
    semantic_buffer_size: int = 3
    # semantic_embedding_model: str = "text-embedding-ada-002"
    semantic_embedding_model: str = "gemini-embedding-001"

    sentence_chunk_size: int = 1024
    sentence_chunk_overlap: int = 200

    token_chunk_size: int = 256
    token_chunk_overlap: int = 50


@dataclass
class EvaluationData:
    """Dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ bao gá»“m cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i tham chiáº¿u"""

    questions: List[str]
    reference_answers: List[List[str]]

    def __post_init__(self):
        if len(self.questions) != len(self.reference_answers):
            raise ValueError(
                "Sá»‘ lÆ°á»£ng cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i tham chiáº¿u pháº£i báº±ng nhau"
            )


def get_all_metrics() -> List:
    """
    Láº¥y táº¥t cáº£ metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡

    Returns:
        List: Danh sÃ¡ch cÃ¡c metrics
    """
    return [
        faithfulness,
        answer_relevancy,
        context_recall,
        context_precision,
    ]


class RAGEvaluator:
    """Class chÃ­nh Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ há»‡ thá»‘ng RAG"""

    def __init__(self, config: EvaluationConfig):
        self.config = config
        self.query_engine: Optional[BaseQueryEngine] = None
        self.embedding_model = GoogleGenAIEmbedding(
            model_name="text-multilingual-embedding-002",
            api_key=settings.GEMINI_AI_API_KEY)

        # self.embedding_model = OpenAIEmbedding(
        #     api_key=settings.OPEN_AI_API_KEY,
        #     model_name=settings.EMBEDDING_MODEL_NAME
        # )
        self.llm = GoogleGenAI(api_key=settings.GEMINI_AI_API_KEY, model=self.config.model_name)
        self.build_rag_system(config.documents_path)

    def _create_node_parser(self):
        """
        Táº¡o node parser dá»±a trÃªn cáº¥u hÃ¬nh chunking strategy

        Returns:
            Node parser phÃ¹ há»£p vá»›i strategy Ä‘Æ°á»£c chá»n
        """

        return SemanticSplitterNodeParser(
            embed_model=self.embedding_model,
            breakpoint_percentile_threshold=self.config.semantic_breakpoint_percentile_threshold,
            buffer_size=self.config.semantic_buffer_size
        )

    def build_rag_system(self, documents_path: str = None) -> Optional[BaseQueryEngine]:
        """
        XÃ¢y dá»±ng há»‡ thá»‘ng RAG tá»« thÆ° má»¥c tÃ i liá»‡u vá»›i semantic chunking

        Args:
            documents_path: ÄÆ°á»ng dáº«n tá»›i thÆ° má»¥c chá»©a tÃ i liá»‡u

        Returns:
            BaseQueryEngine: Engine Ä‘á»ƒ thá»±c hiá»‡n truy váº¥n
        """
        try:
            doc_path = documents_path or self.config.documents_path
            if not doc_path:
                raise ValueError("ChÆ°a cung cáº¥p Ä‘Æ°á»ng dáº«n tÃ i liá»‡u")

            logger.info(f"Äang Ä‘á»c tÃ i liá»‡u tá»«: {doc_path}")
            documents = SimpleDirectoryReader(input_files=[doc_path]).load_data()
            logger.info(f"ÄÃ£ Ä‘á»c {len(documents)} tÃ i liá»‡u")

            node_parser = self._create_node_parser()
            logger.info(f"Sá»­ dá»¥ng semantic chunking vá»›i mÃ´ hÃ¬nh: {self.config.semantic_embedding_model}")

            nodes = node_parser.build_semantic_nodes_from_documents(documents)
            logger.info(f"ÄÃ£ táº¡o {len(nodes)} chunks tá»« tÃ i liá»‡u")

            # Log thÃ´ng tin vá» kÃ­ch thÆ°á»›c chunks (Ä‘á»ƒ debug)
            if nodes:
                chunk_sizes = [len(node.text) for node in nodes[:5]]  # Láº¥y 5 chunks Ä‘áº§u
                logger.info(f"KÃ­ch thÆ°á»›c chunks máº«u: {chunk_sizes}")

            # Táº¡o VectorStoreIndex tá»« nodes
            index = VectorStoreIndex(nodes, embed_model=self.embedding_model)

            self.query_engine = index.as_query_engine(llm=self.llm)
            logger.info("ÄÃ£ xÃ¢y dá»±ng thÃ nh cÃ´ng há»‡ thá»‘ng RAG vá»›i semantic chunking")

            return self.query_engine

        except Exception as ex:
            logger.error(f"Lá»—i khi xÃ¢y dá»±ng há»‡ thá»‘ng RAG: {ex}")
            raise

    def create_sample_evaluation_data(self) -> EvaluationData:
        """
        Táº¡o dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ máº«u tá»« ná»™i dung chÆ°Æ¡ng 1 - Cáº¥u trÃºc dá»¯ liá»‡u vÃ  giáº£i thuáº­t

        Returns:
            EvaluationData: Dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ máº«u
        """
        sample_questions = [
            "Giáº£i thuáº­t lÃ  gÃ¬?",
            "CÃ¡c tÃ­nh cháº¥t cÆ¡ báº£n cá»§a má»™t giáº£i thuáº­t lÃ  gÃ¬?",
            "Dá»¯ liá»‡u vÃ o, trung gian vÃ  ra trong bÃ i toÃ¡n tÃ­nh há»c bá»•ng gá»“m nhá»¯ng gÃ¬?",
            "Cáº¥u trÃºc dá»¯ liá»‡u lÃ  gÃ¬?",
            "Táº¡i sao cáº§n xÃ©t má»‘i quan há»‡ giá»¯a cáº¥u trÃºc dá»¯ liá»‡u vÃ  giáº£i thuáº­t?",
        ]

        sample_answers = [
            [
                "Giáº£i thuáº­t lÃ  má»™t há»‡ thá»‘ng cÃ¡c thao tÃ¡c, cÃ¡c phÃ©p toÃ¡n Ä‘Æ°á»£c thá»±c hiá»‡n theo trÃ¬nh tá»± nháº¥t Ä‘á»‹nh trÃªn má»™t sá»‘ Ä‘á»‘i tÆ°á»£ng dá»¯ liá»‡u nÃ o Ä‘Ã³, sao cho sau má»™t sá»‘ bÆ°á»›c há»¯u háº¡n ta cÃ³ Ä‘Æ°á»£c káº¿t quáº£ mong muá»‘n."
            ],
            [
                "TÃ­nh thá»±c hiá»‡n Ä‘Æ°á»£c, tÃ­nh káº¿t thÃºc, tÃ­nh káº¿t quáº£, tÃ­nh hiá»‡u quáº£, tÃ­nh duy nháº¥t, tÃ­nh tá»•ng quÃ¡t vÃ  tÃ­nh hÃ¬nh thá»©c."
            ],
            [
                "Dá»¯ liá»‡u vÃ o gá»“m há» tÃªn, Ä‘iá»ƒm cÃ¡c mÃ´n, sá»‘ tÃ­n chá»‰; dá»¯ liá»‡u trung gian lÃ  Ä‘iá»ƒm trung bÃ¬nh; dá»¯ liá»‡u ra lÃ  há»c bá»•ng."
            ],
            [
                "Cáº¥u trÃºc dá»¯ liá»‡u lÃ  táº­p há»£p cÃ¡c pháº§n tá»­ dá»¯ liá»‡u liÃªn káº¿t vá»›i nhau báº±ng má»™t cÃ¡ch nÃ o Ä‘Ã³, thá»ƒ hiá»‡n cÃ¡ch tá»• chá»©c dá»¯ liá»‡u trong bá»™ nhá»›."
            ],
            [
                "VÃ¬ giáº£i thuáº­t tÃ¡c Ä‘á»™ng lÃªn cáº¥u trÃºc dá»¯ liá»‡u vÃ  cáº¥u trÃºc dá»¯ liá»‡u áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡ch thiáº¿t káº¿ giáº£i thuáº­t."
            ],
        ]

        return EvaluationData(
            questions=sample_questions, reference_answers=sample_answers
        )

    def evaluate_rag_system(
            self, evaluation_data: EvaluationData, metrics: Optional[List] = None
    ) -> pd.DataFrame:
        """
        ÄÃ¡nh giÃ¡ há»‡ thá»‘ng RAG vá»›i dá»¯ liá»‡u vÃ  metrics Ä‘Æ°á»£c cung cáº¥p

        Args:
            evaluation_data: Dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡
            metrics: Danh sÃ¡ch metrics (náº¿u None sáº½ sá»­ dá»¥ng táº¥t cáº£)

        Returns:
            pd.DataFrame: Káº¿t quáº£ Ä‘Ã¡nh giÃ¡
        """

        if not self.query_engine:
            raise ValueError(
                "ChÆ°a xÃ¢y dá»±ng há»‡ thá»‘ng RAG. Gá»i build_rag_system() trÆ°á»›c."
            )

        if metrics is None:
            metrics = get_all_metrics()

        logger.info(f"Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ vá»›i {len(evaluation_data.questions)} cÃ¢u há»i")
        logger.info(f"Sá»­ dá»¥ng {len(metrics)} metrics")

        try:
            dataset = []

            for query, reference in zip(evaluation_data.questions, evaluation_data.reference_answers):
                dataset.append(
                    {
                        "user_input": query,
                        "reference": reference[0] if isinstance(reference, list) else reference,
                    }
                )

            dataset = EvaluationDataset.from_list(dataset)
            # Thá»±c hiá»‡n Ä‘Ã¡nh giÃ¡
            result = evaluate(
                query_engine=self.query_engine,
                dataset=dataset,
                metrics=metrics,
                llm=self.llm,
                embeddings=self.embedding_model
            )

            # Chuyá»ƒn Ä‘á»•i káº¿t quáº£ thÃ nh DataFrame
            df_result = result.to_pandas()

            # ThÃªm timestamp
            df_result["evaluation_timestamp"] = datetime.now()

            logger.info("ÄÃ¡nh giÃ¡ hoÃ n thÃ nh thÃ nh cÃ´ng")
            return df_result

        except Exception as e:
            logger.error(f"Lá»—i trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡: {e}")
            raise

    def save_results(self, results: pd.DataFrame, output_path: Optional[str] = None):
        """
        LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ ra file CSV

        Args:
            results: DataFrame chá»©a káº¿t quáº£
            output_path: ÄÆ°á»ng dáº«n file output (náº¿u None sáº½ dÃ¹ng config)
        """
        output_file = output_path or self.config.output_csv_path

        try:
            results.to_csv(output_file, ";", encoding="utf-8-sig", index=False)
            logger.info(f"ÄÃ£ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ o: {output_file}")
        except Exception as e:
            logger.error(f"Lá»—i khi lÆ°u file: {e}")
            raise

    def generate_evaluation_report(self, results: pd.DataFrame) -> Dict[str, Any]:
        """
        Táº¡o bÃ¡o cÃ¡o tÃ³m táº¯t káº¿t quáº£ Ä‘Ã¡nh giÃ¡

        Args:
            results: DataFrame chá»©a káº¿t quáº£ Ä‘Ã¡nh giÃ¡

        Returns:
            Dict: BÃ¡o cÃ¡o tÃ³m táº¯t
        """
        numeric_columns = results.select_dtypes(include=["float64", "int64"]).columns

        report = {
            "overview": {
                "total_questions": len(results),
                "evaluation_date": datetime.now().isoformat(),
                "metrics_evaluated": list(numeric_columns),
            },
            "summary_statistics": {
                col: {
                    "mean": float(results[col].mean()),
                    "std": float(results[col].std()),
                    "min": float(results[col].min()),
                    "max": float(results[col].max()),
                    "median": float(results[col].median()),
                }
                for col in numeric_columns
            },
            "performance_insights": self._analyze_performance(
                results, list(numeric_columns)
            ),
        }

        return report

    def _analyze_performance(
            self, results: pd.DataFrame, metric_columns: List[str]
    ) -> Dict[str, str]:
        """
        PhÃ¢n tÃ­ch hiá»‡u suáº¥t dá»±a trÃªn káº¿t quáº£

        Args:
            results: DataFrame chá»©a káº¿t quáº£
            metric_columns: Danh sÃ¡ch tÃªn cá»™t metrics

        Returns:
            Dict: PhÃ¢n tÃ­ch hiá»‡u suáº¥t
        """
        insights = {}

        for col in metric_columns:
            mean_score = results[col].mean()
            if mean_score >= 0.8:
                insights[col] = "Excellent performance"
            elif mean_score >= 0.6:
                insights[col] = "Good performance"
            elif mean_score >= 0.4:
                insights[col] = "Average performance - needs improvement"
            else:
                insights[col] = "Poor performance - requires significant improvement"

        return insights


def run_sample_evaluation():
    """
    Cháº¡y Ä‘Ã¡nh giÃ¡ máº«u vá»›i dá»¯ liá»‡u demo

    ÄÃ¢y lÃ  hÃ m demo Ä‘á»ƒ kiá»ƒm tra hoáº¡t Ä‘á»™ng cá»§a framework
    """
    # Cáº¥u hÃ¬nh Ä‘Ã¡nh giÃ¡
    config = EvaluationConfig(
        model_api_key=settings.OPEN_AI_API_KEY,
        documents_path="document.pdf",
        output_csv_path="rag_evaluation.csv",
        semantic_embedding_model=settings.EMBEDDING_MODEL_NAME,
    )

    try:
        evaluator = RAGEvaluator(config)

        eval_data = evaluator.create_sample_evaluation_data()
        evaluation_result = evaluator.evaluate_rag_system(eval_data)
        evaluator.save_results(evaluation_result)

        # Hiá»ƒn thá»‹ cÃ¢u há»i máº«u
        print("\nğŸ“‹ CÃ¢u há»i máº«u:")
        for i, question in enumerate(eval_data.questions[:3], 1):
            print(f"   {i}. {question}")
        print("   ...")

    except Exception as e:
        print(f"âŒ Lá»—i: {e}")


if __name__ == "__main__":
    print("ğŸš€ RAG Evaluation Framework")
    print("=" * 50)
    run_sample_evaluation()
